---
title: "vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(blblm)
library(tidyverse)
```

# An Overview of This Package

## Goal:

This Package is meant for implementing several different linear models with the the Bag of Little Bootstraps (BLB) algorithm, with the option that allowing user to using parallel computation. The function will return the user the coefficient of the model and also the confidence interval generated through bootstrapy apporach. Under the linear context, the model also enable the user to directly fit their functions. By doing so, we hope this package can provide the user a relatively easier way to deal with large dataset.

## What and Why is BLB?

### Ideas behind the BLB:

The Bag of Little Bootstraps algorithm is an attempt to divide the large dataset into smaller subsamples through bootstrap resampling technique and conduct the computation at the micro-level and then take the average of the statistics computated at each level to get an inference of the parameters we hope to get. Given the modern context in which people often needs to handle relatively large dataset, the traditional bootstrap may seems memory and time consuming, as the resampling of large dataset itself will cost many resources. By divding the data into smaller sets and conducting resampling on the smaller levels, the memory and time cost will be lower than conducting the bootstrap directly on the original dataset.

For the detailed information regarding the BLB algorithm, see Ariel Kleiner el 2011 for the first version of the BLB algorithm. This alogrithm, since invented, recieved wide notices and still being developeds. More detailed and recent examples of the application of BLB algorithm includes de Vi√±a el 2018 on BLB in Enseble learning.

### Why BLB?

However, one may well argue that the BLB algorithm is still memory and time consuming than other algorithm such as the tradition divide and conquer strategy. While this is true, BLB algorithm still having some powerful features than other traditional parallel strategies in dealing with the large dataset. The first of all is that it enables the use of the bootstrap in the parallel process. The traditional divide and conquer strategy, in fact, hardly address the situation that the expectation of the probility is unstable and thus make the ability for each subunit to coverge on the original distribution weaker. In stead of directly dividing the data, by introducing resampling strategy during the partition, we can try to simulate the original dataset at the smaller level and thus granttee the robustness of the computation and thus the inference.

## The structure of the package

### Logical Structure

The package fit the linear model model with the BLB alogrithm by three parts nested inside each of the three linear model this package contain. The first part is the data prepartion which manage the data into proper form and then split the data. The second part is to generate the weights and fit the weighted regression on each subsamples, and the last one is the to compute the coeifficiences of the model in general by summarizing the statistics computed at each mirco-level.

### File organization

This package actually divide the R script by the model used. There are three models contained within this package. The first one is the `lm` version of regression model, the second one is the `glm`, a.k.a, the generalized linear model, and the `rlm`, which refer to the robust linear model introduced by the package `MASS`, and follows the same ideas of the `line` functions from the `stats` package. Each model has a linear version and non-linear version, with the shared first and the third parts nested in `lm`. `glm` has its own verision of coeifficences computation.

# Explanation of the Functions inside the package

## The data handling part

These part of the function deals the input data with two goals. Frist of the all, if the user want to only load part of the data under the working directory. Then, the `read_list` function will load the files matched excatly same name pattern and row_bind them into a new data frame without loading the data as a whole. The function is below. 
Notice, please assign the working directory to the where the data are located.
```{r eval = FALSE}
#' @param data The input data
read_list <- function(data) {
  pattern = paste0(data, collapse = "|") # Translating name into the regExpress
  # Read all files under working directory at once
  files <- list.files(path = getwd(), pattern = pattern, full.names = T) %>% 
    map_df(~fread(.))
  files # Returning the file
}
```


Second, we want to further split the dataset into given number of the subsample and on which, we will achive the bootstraps. The function is below with an example.

```{r}
split_data <- function(data, m) {
  idx <- sample.int(m, nrow(data), replace = TRUE)
  data %>% split(idx)
}
split_data(mtcars,m = 10) %>% # Split the data randomly into 10 subsample
  head()
```

After spiliting data, we now can conduct the bootstrap at each subsample.

## The bootstrap fitting

Here we now start to deal with the fitting process. In general, the fitting process we really on is to, firstly, decided the weights of each boot using a multinominal distribution and then using tradition fit techiques to fit the mode and sumarize them up.

### ordinary linear model and the robust linear model(parallely and non-parallely)

The functional structure for both `lm` fitting and the `rlm` fitting are quit simialr, so here we combine them together. We will first need to use the core function to start the bootsrap fitting, with also an parallel verision functions. For parallel part, this package offer both traditional parallel approch and future map approch for `lm` in separated function.

```{r eval = FALSE}
# Non-parallel version lm
blblm <- function(formula, data, m = 10, B = 5000) {
  if(typeof(data) == "character"){
    df <- read_list(data)
    data_list <- split_data(df, m)
  }
  else{
    data_list <- split_data(data, m)
  }
  estimates <- map(
    data_list,
    ~ lm_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblm"
  invisible(res)
}
```

```{r eval = FALSE}
# Parallel version of rlm
blblm_robust_parallel <- function(formula, data, m = 10, B = 5000, core = 1) {
  cl <- makeCluster(core)
  clusterExport(cl, c("lm_robust_each_boot","lm1_robust","blbcoef","blbsigma"))
  if(typeof(data) == "character"){
    df <- read_list(data)
    data_list <- split_data(df, m)
  }
  else{
    data_list <- split_data(data, m)
  }
  estimates <- parLapplyLB(cl,
                           data_list,
                           chunk.size = 2,
                           lm_robust_each_subsample,formula = formula, n = nrow(data), B = B)
  stopCluster(cl)
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblm"
  invisible(res)
}

blblm_furrr <- function(formula, data, m = 10, B = 5000, core = 1) {
  future::plan(future::multiprocess, workers = core)
  if(typeof(data) == "character"){
    df <- read_list(data)
    data_list <- split_data(df, m)
  }
  else{
    data_list <- split_data(data, m)
  }
  estimates <- future_map(
    data_list,
    ~ lm_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblm"
  invisible(res)
  closeAllConnections()
}
```

Here we can see, we simple pass the parameter such as the number of subsamples, the cores we want to use and the times of Bootstrap we wnat to have and let the function to pass the parameter on to the subsample level. Then we will exam the subsample level. As the following function suggest, here we will repeat the really fit boot process for given times. 

```{r eval = FALSE}
lm_each_subsample <- function(formula, data, n, B) {
  replicate(B, lm_each_boot(formula, data, n), simplify = FALSE)
}
```

Then we will exam what happens at the boot level.

```{r eval = FALSE}
lm_each_boot <- function(formula, data, n) {
  freqs <- rmultinom(1, n, rep(1, nrow(data)))
  lm1(formula, data, freqs)
}

lm1 <- function(formula, data, freqs) {
  # drop the original closure of formula,
  # otherwise the formula will pick a wront variable from the global scope.
  environment(formula) <- environment()
  fit <- lm(formula, data, weights = freqs)
  list(coef = blbcoef(fit), sigma = blbsigma(fit))
}
```

Similarly, for `rlm` we have
```{r eval = FALSE}
lm_robust_each_subsample <- function(formula, data, n, B) {
  replicate(B, lm_each_boot(formula, data, n), simplify = FALSE)
}

lm_robust_each_boot <- function(formula, data, n) {
  freqs <- rmultinom(1, n, rep(1, nrow(data)))
  lm1(formula, data, freqs)
}

lm1_robust <- function(formula, data, freqs) {
  # drop the original closure of formula,
  # otherwise the formula will pick a wront variable from the global scope.
  environment(formula) <- environment()
  fit <- rlm(formula, data, weights = freqs)
  list(coef = blbcoef(fit), sigma = blbsigma(fit))
}
```

As we can see here, we use the mutinominal distribution to decide the weights for each fit and pass to the lm function. The reason we use weight fit is to prevent the different covariance matrix which is required by ordinary fitting.

### glm ftting

For glm, as the user may want to assign the family used for the fitting, a slight modification happened to allow user to pass the family to the bottom level. The user now can assign the family model they want and it will pass by functions to the glm model at the bottom level
```{r eval = FALSE}
blbglm_prallel <- function(formula, data, family = 'gaussian', m = 10, B = 5000, core = 1) {
  cl <- makeCluster(core)
  clusterExport(cl, c("glm_each_boot","glm_base","blbcoef","blbsigma"))
  if(typeof(data) == "character"){
    df <- read_list(data)
    data_list <- split_data(df, m)
  }
  else{
    data_list <- split_data(data, m)
  }
  estimates <- parLapplyLB(cl,
                           data_list,
                           chunk.size = 2,
                           glm_each_subsample,formula = formula, family = family, n = nrow(data), B = B)
  stopCluster(cl)
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblm"
  invisible(res)
}
# Bottom level
glm_base <- function(formula, family, data, freqs) {
  # drop the original closure of formula,
  # otherwise the formula will pick a wront variable from the global scope.
  environment(formula) <- environment()
  fit <- glm(formula, family = family, data, weights = freqs)
  list(coef = blbcoef(fit), sigma = blbsigma(fit))
 }
```


## Summarizing the statistics and other functions

### Computing and Summarizing the statistics for each boot
However, one may well notices that there is certain functions at the very bottom level of each model such as `blbcoef`, `sigma` etc. Those are the functions that we would like to use to acquire the coefficiences of each fitting. Those functions records the coefficiences at the bottom levels and return in the form of the list. Then at a higher level, the list is reduced through `map_reduce` function and then stored up. Allowing user to use functions to access. The technique details are bellow.

```{r eval = FALSE}
mean_lwr_upr <- function(x, level = 0.95) {
  alpha <- 1 - level
  c(fit = mean(x), quantile(x, c(alpha / 2, 1 - alpha / 2)) %>% set_names(c("lwr", "upr")))
}

map_mean <- function(.x, .f, ...) {
  (map(.x, .f, ...) %>% reduce(`+`)) / length(.x)
}

map_cbind <- function(.x, .f, ...) {
  map(.x, .f, ...) %>% reduce(cbind)
}

map_rbind <- function(.x, .f, ...) {
  map(.x, .f, ...) %>% reduce(rbind)
}
```



### Statistics user can acess

There are mainly two set of parameters user can access at the top level after the fitting. The first set is the coefficiences (in average form) of each subsample. The second is the sigma, in average form of each model. There acquire through follow functions. For several of the them, user can have the confidence interval than mere parameter.

```{r eval = FALSE}
sigma.blblm <- function(object, confidence = FALSE, level = 0.95, ...) {
  est <- object$estimates
  sigma <- mean(map_dbl(est, ~ mean(map_dbl(., "sigma"))))
  if (confidence) {
    alpha <- 1 - 0.95
    limits <- est %>%
      map_mean(~ quantile(map_dbl(., "sigma"), c(alpha / 2, 1 - alpha / 2))) %>%
      set_names(NULL)
    return(c(sigma = sigma, lwr = limits[1], upr = limits[2]))
  } else {
    return(sigma)
  }
}
  
coef.blblm <- function(object, ...) {
  est <- object$estimates
  map_mean(est, ~ map_cbind(., "coef") %>% rowMeans())
}

confint.blblm <- function(object, parm = NULL, level = 0.95, ...) {
  if (is.null(parm)) {
    parm <- attr(terms(fit$formula), "term.labels")
  }
  alpha <- 1 - level
  est <- object$estimates
  out <- map_rbind(parm, function(p) {
    map_mean(est, ~ map_dbl(., list("coef", p)) %>% quantile(c(alpha / 2, 1 - alpha / 2)))
  })
  if (is.vector(out)) {
    out <- as.matrix(t(out))
  }
  dimnames(out)[[1]] <- parm
  out
}
```

### Prediction

The user can input they new data and predict it using predict `function` also. The data will be convert to a design matrix and fit.

```{r eval = FALSE}
predict.blblm <- function(object, new_data, confidence = FALSE, level = 0.95,core = 1, ...) {
  est <- object$estimates
  X <- model.matrix(reformulate(attr(terms(object$formula), "term.labels")), new_data)
  if (confidence) {
    map_mean_parallel(est, ~ map_cbind(., ~ X %*% .$coef) %>%
      apply(1, mean_lwr_upr, level = level) %>%
      t())
  } else {
    map_mean(est, ~ map_cbind(., ~ X %*% .$coef) %>% rowMeans())
  }
}
```


# An round up example using lm
```{r warning=FALSE}
fit <- blblm(mpg ~ wt * hp, data = mtcars, m = 3, B = 100)
coef(fit)
```

```{r}
confint(fit, c("wt", "hp"))
```

```{r}
sigma(fit)
sigma(fit, confidence = TRUE)
```


